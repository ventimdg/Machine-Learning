{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function Implementations:\n",
    "\n",
    "Implementation of `activations.Linear`:\n",
    "\n",
    "```python\n",
    "class Linear(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for f(z) = z.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z  input pre-activations (any shape)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f(z) as described above applied elementwise to `Z`\n",
    "        \"\"\"\n",
    "        return Z\n",
    "\n",
    "    def backward(self, Z: np.ndarray, dY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for f(z) = z.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z   input to `forward` method\n",
    "        dY  derivative of loss w.r.t. the output of this layer\n",
    "            same shape as `Z`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        derivative of loss w.r.t. input of this layer\n",
    "        \"\"\"\n",
    "        return dY\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `activations.Sigmoid`:\n",
    "\n",
    "```python\n",
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for sigmoid function:\n",
    "        f(z) = 1 / (1 + exp(-z))\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z  input pre-activations (any shape)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f(z) as described above applied elementwise to `Z`\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        return ...\n",
    "\n",
    "    def backward(self, Z: np.ndarray, dY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for sigmoid.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z   input to `forward` method\n",
    "        dY  derivative of loss w.r.t. the output of this layer\n",
    "            same shape as `Z`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        derivative of loss w.r.t. input of this layer\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        return ...\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `activations.ReLU`:\n",
    "\n",
    "```python\n",
    "class ReLU(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for relu activation:\n",
    "        f(z) = z if z >= 0\n",
    "               0 otherwise\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z  input pre-activations (any shape)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f(z) as described above applied elementwise to `Z`\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def backward(self, Z: np.ndarray, dY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for relu activation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z   input to `forward` method\n",
    "        dY  derivative of loss w.r.t. the output of this layer\n",
    "            same shape as `Z`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        derivative of loss w.r.t. input of this layer\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        dY[Z < 0] = 0\n",
    "        return dY\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `activations.SoftMax`:\n",
    "\n",
    "```python\n",
    "class SoftMax(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for softmax activation.\n",
    "        Hint: The naive implementation might not be numerically stable.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z  input pre-activations (any shape)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f(z) as described above applied elementwise to `Z`\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "        maxes = np.array(np.max(Z, axis=1))\n",
    "        maxes = np.reshape(maxes, (np.shape(maxes)[0], 1))\n",
    "        \n",
    "        pre_exponential =  Z - maxes\n",
    "        post_exponential = np.exp(pre_exponential)\n",
    "\n",
    "        sums = np.sum(post_exponential, axis=1)\n",
    "        sums = np.reshape(sums, (np.shape(sums)[0], 1))\n",
    "\n",
    "        answer = np.divide(post_exponential, sums)\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def backward(self, Z: np.ndarray, dY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for softmax activation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z   input to `forward` method\n",
    "        dY  derivative of loss w.r.t. the output of this layer\n",
    "            same shape as `Z`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        derivative of loss w.r.t. input of this layer\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        forward = self.forward(Z)\n",
    "        answer = np.zeros_like(Z)\n",
    "        for i in range(np.shape(Z)[0]):\n",
    "            data =  forward[i]\n",
    "            outer = np.multiply(-1, np.outer(data, data))\n",
    "            diagonal = np.diag(outer)\n",
    "            diagonal= np.multiply(-1, diagonal)\n",
    "            diagonal = np.sqrt(diagonal)\n",
    "            diagonal = np.diag(diagonal)\n",
    "            jacobian = diagonal + outer\n",
    "            answer[i] = dY[i] @ jacobian\n",
    "        return answer\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Layer Implementations:\n",
    "\n",
    "Implementation of `layers.FullyConnected`:\n",
    "\n",
    "```python\n",
    "class FullyConnected(Layer):\n",
    "    \"\"\"A fully-connected layer multiplies its input by a weight matrix, adds\n",
    "    a bias, and then applies an activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, n_out: int, activation: str, weight_init=\"xavier_uniform\"\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_in = None\n",
    "        self.n_out = n_out\n",
    "        self.activation = initialize_activation(activation)\n",
    "\n",
    "        # instantiate the weight initializer\n",
    "        self.init_weights = initialize_weights(weight_init, activation=activation)\n",
    "\n",
    "    def _init_parameters(self, X_shape: Tuple[int, int]) -> None:\n",
    "        \"\"\"Initialize all layer parameters (weights, biases).\"\"\"\n",
    "        self.n_in = X_shape[1]\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        W = self.init_weights((self.n_in, self.n_out))\n",
    "        b = np.zeros((1, self.n_out))\n",
    "\n",
    "        self.parameters = OrderedDict({\"W\": W, \"b\": b})\n",
    "        self.cache: OrderedDict = OrderedDict()  # cache for backprop\n",
    "        self.gradients: OrderedDict = OrderedDict({\"W\": np.zeros_like(W), \"b\": np.zeros_like(b)})  # parameter gradients initialized to zero\n",
    "                                           # MUST HAVE THE SAME KEYS AS `self.parameters`\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass: multiply by a weight matrix, add a bias, apply activation.\n",
    "        Also, store all necessary intermediate results in the `cache` dictionary\n",
    "        to be able to compute the backward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input matrix of shape (batch_size, input_dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a matrix of shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        # initialize layer parameters if they have not been initialized\n",
    "        if self.n_in is None:\n",
    "            self._init_parameters(X.shape)\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        W = self.parameters[\"W\"]\n",
    "        b = self.parameters[\"b\"]\n",
    "        Z = X @ W + b\n",
    "\n",
    "        self.cache[\"X\"] = X\n",
    "        self.cache[\"Z\"] = Z\n",
    "        \n",
    "        # perform an affine transformation and activation\n",
    "        out = self.activation(Z)\n",
    "        \n",
    "        # store information necessary for backprop in `self.cache`\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dLdY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for fully connected layer.\n",
    "        Compute the gradients of the loss with respect to:\n",
    "            1. the weights of this layer (mutate the `gradients` dictionary)\n",
    "            2. the bias of this layer (mutate the `gradients` dictionary)\n",
    "            3. the input of this layer (return this)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dLdY  derivative of the loss with respect to the output of this layer\n",
    "              shape (batch_size, output_dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        derivative of the loss with respect to the input of this layer\n",
    "        shape (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "        \n",
    "        # unpack the cache\n",
    "        X = self.cache[\"X\"] \n",
    "        Z = self.cache[\"Z\"]\n",
    "\n",
    "        # compute the gradients of the loss w.r.t. all parameters as well as the\n",
    "        # input of the layer\n",
    "\n",
    "        W = self.parameters[\"W\"]\n",
    "        \n",
    "        dLdZ = self.activation.backward(Z, dLdY)\n",
    "        dLdX = dLdZ @ W.T\n",
    "        dLdW = X.T @ dLdZ\n",
    "        dLdb = np.ones((1, np.shape(dLdZ)[0])) @ dLdZ\n",
    "\n",
    "\n",
    "        # store the gradients in `self.gradients`\n",
    "        # the gradient for self.parameters[\"W\"] should be stored in\n",
    "        # self.gradients[\"W\"], etc.\n",
    "\n",
    "\n",
    "        self.gradients[\"W\"] = dLdW\n",
    "        self.gradients[\"b\"] = dLdb\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return dLdX\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `layers.Pool2D`:\n",
    "\n",
    "```python\n",
    "class Pool2D(Layer):\n",
    "    \"\"\"Pooling layer, implements max and average pooling.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_shape: Tuple[int, int],\n",
    "        mode: str = \"max\",\n",
    "        stride: int = 1,\n",
    "        pad: Union[int, Literal[\"same\"], Literal[\"valid\"]] = 0,\n",
    "    ) -> None:\n",
    "\n",
    "        if type(kernel_shape) == int:\n",
    "            kernel_shape = (kernel_shape, kernel_shape)\n",
    "\n",
    "        self.kernel_shape = kernel_shape\n",
    "        self.stride = stride\n",
    "\n",
    "        if pad == \"same\":\n",
    "            self.pad = ((kernel_shape[0] - 1) // 2, (kernel_shape[1] - 1) // 2)\n",
    "        elif pad == \"valid\":\n",
    "            self.pad = (0, 0)\n",
    "        elif isinstance(pad, int):\n",
    "            self.pad = (pad, pad)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Pad mode found in self.pad.\")\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == \"max\":\n",
    "            self.pool_fn = np.max\n",
    "            self.arg_pool_fn = np.argmax\n",
    "        elif mode == \"average\":\n",
    "            self.pool_fn = np.mean\n",
    "\n",
    "        self.cache = {\n",
    "            \"out_rows\": [],\n",
    "            \"out_cols\": [],\n",
    "            \"X_pad\": [],\n",
    "            \"p\": [],\n",
    "            \"pool_shape\": [],\n",
    "        }\n",
    "        self.parameters = {}\n",
    "        self.gradients = {}\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass: use the pooling function to aggregate local information\n",
    "        in the input. This layer typically reduces the spatial dimensionality of\n",
    "        the input while keeping the number of feature maps the same.\n",
    "\n",
    "        As with all other layers, please make sure to cache the appropriate\n",
    "        information for the backward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input array of shape (batch_size, in_rows, in_cols, channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pooled array of shape (batch_size, out_rows, out_cols, channels)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        # implement the forward pass\n",
    "\n",
    "        n_examples, in_rows, in_cols, in_channels = X.shape\n",
    "        kernel_height, kernel_width = self.kernel_shape[0], self.kernel_shape[1]\n",
    "\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        # implement a convolutional forward pass\n",
    "\n",
    "        padded = np.pad(X, ((0, 0), (self.pad[0], self.pad[0]), (self.pad[1], self.pad[1]), (0, 0)), mode= 'constant')\n",
    "\n",
    "        padded_rows  = in_rows + 2*self.pad[0]\n",
    "        padded_cols = in_cols + 2*self.pad[1]\n",
    "        filtered_rows = padded_rows - kernel_height\n",
    "        filtered_cols = padded_cols - kernel_width\n",
    "\n",
    "        num_output_rows = int(filtered_rows / self.stride + 1) \n",
    "        num_output_cols  = int(filtered_cols / self.stride + 1)\n",
    "\n",
    "        \n",
    "        answer = np.zeros((n_examples, num_output_rows, num_output_cols, in_channels)) \n",
    "        \n",
    "        for row in range(num_output_rows):\n",
    "            \n",
    "            for col in range(num_output_cols):\n",
    "\n",
    "                padded_slice = padded[:, row * self.stride : row * self.stride + kernel_height, col * self.stride : col * self.stride + kernel_width, :]\n",
    "\n",
    "                answer[:, row, col, :] = self.pool_fn(padded_slice,  axis=(1, 2))\n",
    "        \n",
    "        \n",
    "        self.cache[\"out_rows\"] = num_output_rows\n",
    "        self.cache[\"out_cols\"] = num_output_cols\n",
    "        self.cache[\"in_rows\"] = in_rows\n",
    "        self.cache[\"in_cols\"] = in_cols\n",
    "        self.cache[\"X_pad\"] = padded\n",
    "\n",
    "        # cache any values required for backprop\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def backward(self, dLdY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for pooling layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dLdY  gradient of loss with respect to the output of this layer\n",
    "              shape (batch_size, out_rows, out_cols, channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient of loss with respect to the input of this layer\n",
    "        shape (batch_size, in_rows, in_cols, channels)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        # perform a backward pass\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "    \n",
    "\n",
    "        padded = self.cache[\"X_pad\"]\n",
    "        num_output_rows = self.cache[\"out_rows\"]\n",
    "        num_output_cols = self.cache[\"out_cols\"]\n",
    "        in_rows = self.cache[\"in_rows\"] \n",
    "        in_cols = self.cache[\"in_cols\"] \n",
    "        kernel_height, kernel_width = self.kernel_shape[0], self.kernel_shape[1]\n",
    "        average_div = kernel_height * kernel_width\n",
    "        padded_out = np.zeros_like(padded)\n",
    "\n",
    "\n",
    "        for row in range(num_output_rows):\n",
    "            for col in range(num_output_cols):\n",
    "\n",
    "                if self.mode == \"average\":\n",
    "                    \n",
    "                    derivative =  dLdY[:, row : row + 1, col : col + 1, :] / average_div\n",
    "\n",
    "                    padded_out[:, row * self.stride : row * self.stride + kernel_height, col * self.stride : col * self.stride + kernel_width, :] += derivative\n",
    "\n",
    "                if self.mode == \"max\":\n",
    "\n",
    "                    padded_slice = padded[:, row * self.stride : row * self.stride + kernel_height, col * self.stride : col * self.stride + kernel_width, :]\n",
    "\n",
    "                    flattened_spatial = padded_slice.reshape(padded_slice.shape[0], -1, padded_slice.shape[3]) \n",
    "\n",
    "                    removed = (flattened_spatial == np.max(flattened_spatial, axis=1, keepdims=True))\n",
    "\n",
    "                    removed = removed.reshape(padded_slice.shape[0], kernel_height, kernel_width, padded_slice.shape[3])\n",
    "\n",
    "                    padded_out[:, row * self.stride : row * self.stride + kernel_height , col * self.stride : col * self.stride + kernel_width, :] += dLdY[:, row:row+1, col:col+1, :] * removed\n",
    "        \n",
    "        return padded_out[:, self.pad[0]:in_rows+self.pad[0], self.pad[1]:in_cols+self.pad[1], :] \n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `layers.Conv2D.__init__`:\n",
    "\n",
    "```python\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_out: int,\n",
    "        kernel_shape: Tuple[int, int],\n",
    "        activation: str,\n",
    "        stride: int = 1,\n",
    "        pad: str = \"same\",\n",
    "        weight_init: str = \"xavier_uniform\",\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_in = None\n",
    "        self.n_out = n_out\n",
    "        self.kernel_shape = kernel_shape\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "        self.activation = initialize_activation(activation)\n",
    "        self.init_weights = initialize_weights(weight_init, activation=activation)\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `layers.Conv2D._init_parameters`:\n",
    "\n",
    "```python\n",
    "    def _init_parameters(self, X_shape: Tuple[int, int, int, int]) -> None:\n",
    "        \"\"\"Initialize all layer parameters and determine padding.\"\"\"\n",
    "        self.n_in = X_shape[3]\n",
    "\n",
    "        W_shape = self.kernel_shape + (self.n_in,) + (self.n_out,)\n",
    "        W = self.init_weights(W_shape)\n",
    "        b = np.zeros((1, self.n_out))\n",
    "\n",
    "        self.parameters = OrderedDict({\"W\": W, \"b\": b})\n",
    "        self.cache = OrderedDict({\"Z\": [], \"X\": []})\n",
    "        self.gradients = OrderedDict({\"W\": np.zeros_like(W), \"b\": np.zeros_like(b)})\n",
    "\n",
    "        if self.pad == \"same\":\n",
    "            self.pad = ((W_shape[0] - 1) // 2, (W_shape[1] - 1) // 2)\n",
    "        elif self.pad == \"valid\":\n",
    "            self.pad = (0, 0)\n",
    "        elif isinstance(self.pad, int):\n",
    "            self.pad = (self.pad, self.pad)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Pad mode found in self.pad.\")\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `layers.Conv2D.forward`:\n",
    "\n",
    "```python\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for convolutional layer. This layer convolves the input\n",
    "        `X` with a filter of weights, adds a bias term, and applies an activation\n",
    "        function to compute the output. This layer also supports padding and\n",
    "        integer strides. Intermediates necessary for the backward pass are stored\n",
    "        in the cache.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input with shape (batch_size, in_rows, in_cols, in_channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output feature maps with shape (batch_size, out_rows, out_cols, out_channels)\n",
    "        \"\"\"\n",
    "        if self.n_in is None:\n",
    "            self._init_parameters(X.shape)\n",
    "\n",
    "        W = self.parameters[\"W\"]\n",
    "        b = self.parameters[\"b\"]\n",
    "\n",
    "        kernel_height, kernel_width, in_channels, out_channels = W.shape\n",
    "        n_examples, in_rows, in_cols, in_channels = X.shape\n",
    "        kernel_shape = (kernel_height, kernel_width)\n",
    "\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        # implement a convolutional forward pass\n",
    "\n",
    "        padded = np.pad(X, ((0, 0), (self.pad[0], self.pad[0]), (self.pad[1], self.pad[1]), (0, 0)), mode= 'constant')\n",
    "\n",
    "        padded_rows  = in_rows + 2*self.pad[0]\n",
    "        padded_cols = in_cols + 2*self.pad[1]\n",
    "        filtered_rows = padded_rows - kernel_height\n",
    "        filtered_cols = padded_cols - kernel_width\n",
    "\n",
    "        num_output_rows = int(filtered_rows / self.stride + 1) \n",
    "        num_output_cols  = int(filtered_cols / self.stride + 1)\n",
    "\n",
    "        \n",
    "        Z = np.zeros((n_examples, num_output_rows, num_output_cols, out_channels)) \n",
    "        \n",
    "        for row in range(num_output_rows):\n",
    "            for col in range(num_output_cols):\n",
    "                for channel in range(out_channels):\n",
    "                    padded_slice = padded[:, row * self.stride : row * self.stride + kernel_height, col * self.stride : col * self.stride + kernel_width, :]\n",
    "                    weight_slice = W[:, :, :, channel]\n",
    "                    convolved = padded_slice * weight_slice\n",
    "                    Z[:, row, col, channel] = np.einsum('ijkl->i', convolved) + b[:, channel]\n",
    "        \n",
    "        self.cache[\"Z\"] = Z \n",
    "        self.cache[\"X\"] = X\n",
    "\n",
    "        # cache any values required for backprop\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "\n",
    "        return self.activation(Z)\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `layers.Conv2D.backward`:\n",
    "\n",
    "```python\n",
    "    def backward(self, dLdY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for conv layer. Computes the gradients of the output\n",
    "        with respect to the input feature maps as well as the filter weights and\n",
    "        biases.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dLdY  derivative of loss with respect to output of this layer\n",
    "              shape (batch_size, out_rows, out_cols, out_channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        derivative of the loss with respect to the input of this layer\n",
    "        shape (batch_size, in_rows, in_cols, in_channels)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        # perform a backward pass\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        X = self.cache[\"X\"]\n",
    "        Z = self.cache[\"Z\"] \n",
    "\n",
    "        W = self.parameters[\"W\"] \n",
    "        \n",
    "        kernel_height, kernel_width, in_channels, out_channels = W.shape \n",
    "        n_examples, in_rows, in_cols, in_channels = X.shape\n",
    "\n",
    "        padded = np.pad(X, ((0, 0), (self.pad[0], self.pad[0]), (self.pad[1], self.pad[1]), (0, 0)), mode= 'constant')\n",
    "\n",
    "        padded_rows  = in_rows + 2*self.pad[0]\n",
    "        padded_cols = in_cols + 2*self.pad[1]\n",
    "        filtered_rows = padded_rows - kernel_height\n",
    "        filtered_cols = padded_cols - kernel_width\n",
    "\n",
    "        num_output_rows = int(filtered_rows / self.stride + 1) \n",
    "        num_output_cols  = int(filtered_cols / self.stride + 1)\n",
    "\n",
    "        dLdZ = self.activation.backward(Z, dLdY)\n",
    "\n",
    "        padded_out = np.zeros_like(padded)\n",
    "        dLdW = np.zeros_like(W)\n",
    "\n",
    "        self.gradients[\"b\"] = np.einsum('ijkl->l', dLdZ)\n",
    "\n",
    "        for row in range(num_output_rows):\n",
    "            for col in range(num_output_cols):\n",
    "                for channel in range(out_channels):\n",
    "\n",
    "                    padded_slice = padded[:, row * self.stride : row * self.stride + kernel_height, col * self.stride : col * self.stride + kernel_width, :]\n",
    "                    weight_slice = W[None, :, :, :, channel]\n",
    "                    dldz_slice =  dLdZ[:, row : row + 1, col : col + 1, None, channel]\n",
    "\n",
    "                    padded_out[:,row * self.stride : row * self.stride + kernel_height, col * self.stride : col * self.stride + kernel_width, :] += weight_slice * dldz_slice\n",
    "\n",
    "                    convolved  = padded_slice * dldz_slice\n",
    "\n",
    "                    dLdW[:, :, :, channel] += np.einsum(\"ijkl->jkl\", convolved)\n",
    "        \n",
    "        self.gradients[\"W\"] = dLdW \n",
    "\n",
    "        return padded_out[:, self.pad[0]:in_rows+self.pad[0], self.pad[1]:in_cols+self.pad[1], :] \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Loss Function Implementations:\n",
    "\n",
    "Implementation of `losses.CrossEntropy`:\n",
    "\n",
    "```python\n",
    "class CrossEntropy(Loss):\n",
    "    \"\"\"Cross entropy loss function.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str) -> None:\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, Y: np.ndarray, Y_hat: np.ndarray) -> float:\n",
    "        return self.forward(Y, Y_hat)\n",
    "\n",
    "    def forward(self, Y: np.ndarray, Y_hat: np.ndarray) -> float:\n",
    "        \"\"\"Computes the loss for predictions `Y_hat` given one-hot encoded labels\n",
    "        `Y`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y      one-hot encoded labels of shape (batch_size, num_classes)\n",
    "        Y_hat  model predictions in range (0, 1) of shape (batch_size, num_classes)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a single float representing the loss\n",
    "        \"\"\"\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "        losses = Y * np.log(Y_hat)\n",
    "        sum_losses = -np.sum(losses)\n",
    "        answer = sum_losses / np.shape(Y)[0]\n",
    "        return answer\n",
    "\n",
    "    def backward(self, Y: np.ndarray, Y_hat: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass of cross-entropy loss.\n",
    "        NOTE: This is correct ONLY when the loss function is SoftMax.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y      one-hot encoded labels of shape (batch_size, num_classes)\n",
    "        Y_hat  model predictions in range (0, 1) of shape (batch_size, num_classes)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        the derivative of the cross-entropy loss with respect to the vector of\n",
    "        predictions, `Y_hat`\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        vector = np.multiply(-1, np.divide(Y, Y_hat))\n",
    "        answer = np.divide(vector, np.shape(Y)[0])\n",
    "        return answer\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `losses.L2`:\n",
    "\n",
    "```python\n",
    "class L2(Loss):\n",
    "    \"\"\"Mean squared error loss.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str) -> None:\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, Y: np.ndarray, Y_hat: np.ndarray) -> float:\n",
    "        return self.forward(Y, Y_hat)\n",
    "\n",
    "    def forward(self, Y: np.ndarray, Y_hat: np.ndarray) -> float:\n",
    "        \"\"\"Compute the mean squared error loss for predictions `Y_hat` given\n",
    "        regression targets `Y`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y      vector of regression targets of shape (batch_size, 1)\n",
    "        Y_hat  vector of predictions of shape (batch_size, 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a single float representing the loss\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        return ...\n",
    "\n",
    "    def backward(self, Y: np.ndarray, Y_hat: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for mean squared error loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y      vector of regression targets of shape (batch_size, 1)\n",
    "        Y_hat  vector of predictions of shape (batch_size, 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        the derivative of the mean squared error with respect to the last layer\n",
    "        of the neural network\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        return ...\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Model Implementations:\n",
    "\n",
    "Implementation of `models.NeuralNetwork.forward`:\n",
    "\n",
    "```python\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"One forward pass through all the layers of the neural network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  design matrix whose must match the input shape required by the\n",
    "           first layer\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        forward pass output, matches the shape of the output of the last layer\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Iterate through the network's layers.\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `models.NeuralNetwork.backward`:\n",
    "\n",
    "```python\n",
    "    def backward(self, target: np.ndarray, out: np.ndarray) -> float:\n",
    "        \"\"\"One backward pass through all the layers of the neural network.\n",
    "        During this phase we calculate the gradients of the loss with respect to\n",
    "        each of the parameters of the entire neural network. Most of the heavy\n",
    "        lifting is done by the `backward` methods of the layers, so this method\n",
    "        should be relatively simple. Also make sure to compute the loss in this\n",
    "        method and NOT in `self.forward`.\n",
    "\n",
    "        Note: Both input arrays have the same shape.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        target  the targets we are trying to fit to (e.g., training labels)\n",
    "        out     the predictions of the model on training data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        the loss of the model given the training inputs and targets\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Compute the loss.\n",
    "        # Backpropagate through the network's layers.\n",
    "        curr_loss = self.loss.forward(target, out)\n",
    "        derivative = self.loss.backward(target, out)\n",
    "        backwards = self.layers[::-1]\n",
    "        for layer in backwards:\n",
    "            derivative = layer.backward(derivative)\n",
    "        return curr_loss\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `models.NeuralNetwork.predict`:\n",
    "\n",
    "```python\n",
    "    def predict(self, X: np.ndarray, Y: np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "        \"\"\"Make a forward and backward pass to calculate the predictions and\n",
    "        loss of the neural network on the given data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input features\n",
    "        Y  targets (same length as `X`)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a tuple of the prediction and loss\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Do a forward pass. Maybe use a function you already wrote?\n",
    "        # Get the loss. Remember that the `backward` function returns the loss.\n",
    "        answer = self.forward(X)\n",
    "        loss = self.backward(Y, answer)\n",
    "        return (answer, loss)\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
